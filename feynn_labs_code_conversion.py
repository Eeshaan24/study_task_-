# -*- coding: utf-8 -*-
"""Feynn_Labs_Code_Conversion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wmlTE2FqjGL9N2VD3oB6R0WKd5L4Yt30
"""

import pandas as pd
import numpy as np

data = pd.read_csv('/content/mcdonalds.csv')



data.columns.values.tolist()

data.shape

data.replace({"yummy":{'No':0, 'Yes':1}}, inplace = True)

data.replace({'convenient':{'No':0, 'Yes':1}, 'spicy' : {'No':0, 'Yes':1}, 'fattening' : {'No':0, 'Yes':1}, 'greasy' : {'No':0, 'Yes':1}, 'fast' : {'No':0, 'Yes':1}, 'cheap' : {'No':0, 'Yes':1}, 'tasty':{'No':0, 'Yes':1}, 'expensive':{'No':0, 'Yes':1}, 'healthy':{'No':0, 'Yes':1}, 'disgusting':{'No':0, 'Yes':1}},  inplace = True)

X = data.iloc[:, 0:11]

print(X)

Y = X.mean(axis=0)

Y.round(2)

from sklearn.decomposition import PCA

pca = PCA()
X_pca = pca.fit(X)

SD=np.sqrt(pca.explained_variance_)
PV=pca.explained_variance_ratio_
outsum = np.cumsum(pca.explained_variance_ratio_)

index=[]
for i in range(len(SD)):
    i=i+1
    index.append("PC{}".format(i))

sum=pd.DataFrame({
    "Standard deviation":SD,"Proportion of Variance":PV,"Cumulative Proportion":outsum
},index=index)
sum

print("Standard Deviation:\n",SD.round(1))

load = (pca.components_)
i=0
rot_matrix = X_pca.components_.T

rot_df = pd.DataFrame(rot_matrix, index=X.columns.values, columns=index)
rot_df=round(-rot_df,3)
rot_df

pip install bioinfokit

import matplotlib.pyplot as plt

X_pred = X_pca.transform(X)
plt.scatter(X_pred[:, 0], X_pred[:, 1], color='grey')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

wcss = []
segments=range(1,9)

for i in segments:
  kmeans = KMeans(n_clusters=i, init='k-means++', random_state=123)
  kmeans.fit(X)

  wcss.append(kmeans.inertia_)

plt.bar(segments, wcss)
plt.xlabel("Number of segments")
plt.ylabel("Sum of within-cluster distances")
plt.title("Segmentation Results")
plt.show()

print(wcss)



Y = []

for i in range(2,9):
  kmeans = KMeans(n_clusters=i, init='k-means++', random_state=123)
  kmeans.fit_predict(X)

  Y.append(kmeans.inertia_)

from sklearn.utils import resample

Y = [1,0,0,1,1,0,1,2,2,2,2]
samples=[]

for _ in range(10):
  K = resample(Y, random_state=123)
  samples.append(K)
print(samples)

from scipy.stats import entropy

np.random.seed(1234)
k_values = range(2, 9)
MD_m28 = []

for k in k_values:
    model = KMeans(n_clusters=k, random_state=1234)
    model.fit(X.values)
    iter_val = model.n_iter_
    converged = True
    k_val = k
    k0_val = k
    log_likelihood = -model.inertia_
    n_samples, _ = X.shape
    aic = -2 * log_likelihood + 2 * k
    bic = -2 * log_likelihood + np.log(n_samples) * k
    labels = model.labels_
    counts = np.bincount(labels)
    probs = counts / float(counts.sum())
    class_entropy = entropy(probs)
    icl = bic - class_entropy

    MD_m28.append((iter_val, converged, k_val, k0_val, log_likelihood, aic, bic, icl))
MD_m28 = pd.DataFrame(MD_m28, columns=['iter', 'converged', 'k', 'k0', 'logLik', 'AIC', 'BIC', 'ICL'])

print(MD_m28)

data.head()

like_counts = data['Like'].value_counts()
like_counts = like_counts.sort_index(ascending=False)
print(like_counts)

like_ = pd.value_counts(data['Like'])
print(like_.iloc[::-1])



mapping = {
    'I hate it!-5': -5,
    '-4': -4,
    '-3': -3,
    '-2': -2,
    '-1': -1,
    '0': 0,
    '1': 1,
    '2': 2,
    '3': 3,
    '4': 4,
    'I love it!+5': 5
}

data['Like'] = data['Like'].map(mapping)


like_counts = data['Like'].value_counts()


print(like_counts)

from patsy import dmatrices

independent_vars = data.columns[0:11]

formula_str = ' + '.join(independent_vars)

formula_str = 'Like ~ ' + formula_str


f = dmatrices(formula_str, data=data)[1]

print(f)

from sklearn.mixture import GaussianMixture
from patsy import dmatrix
np.random.seed(1234)

X = dmatrix(f.design_info, data=data)
y = dmatrix('Like', data=data)

n_components = 2
n_init = 10
verbose = False
n_rep=10

model = GaussianMixture(n_components=n_components, n_init=n_init, verbose=verbose)
MD_reg2 = model.fit(X, y)

print(MD_reg2)
cluster_sizes = np.bincount(model.predict(X))

print("Cluster sizes:")
for i, size in enumerate(cluster_sizes):
    print(f"{i+1}: {size}")



